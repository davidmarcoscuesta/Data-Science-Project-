---
title: "Data Science Project"
author: "David Marcos Cuesta"
date: "2023-12-06"
output: html_document
---

## Libraries
```{r}
library(tidyverse)
library(qdapDictionaries)
library(ggplot2) 
library(readxl)            # Read .xlsx   
library(hexbin) 

knitr::opts_chunk$set(echo = TRUE)

```


## Function to clean data
```{r}
# Function to clean response text
clean_response <- function(data, pattern, cols_to_remove) {
  data %>% 
    select(-all_of(cols_to_remove)) %>% 
    filter(str_detect(response, pattern)) %>%
    mutate(response = str_replace_all(response, '[\"{}]', '')) %>%
    mutate(response = str_remove_all(response, "let_| ")) %>%
    separate(response, c("condition", "response"), sep = ":")
}
```



## Read the data from the raw dataset
```{r}
master_file <- read.csv('data/master_file_01.csv') %>% 
  select(subject_id, stimulus, trial_index, time_elapsed, rt, response)
```


## Define the dictionary of possible English words
```{r}
# Function to check if it's a word in the dictionary

words_not_in_dic = c("zoopathology", "zolt", "zoolagist", "zot", "zock", "zook", "zon",
                     "zosh", "zoze", "zoam", "zommer", "zozo", "zoomies", "zoop", "zope",
                     "zoetrope", "zoocology", "zote", "zoge", "zoed", "zomed", "zork",
                     "zomboni", "zotie", "zolac", "zoz", "zoadic", "zooplankton", "zoro",
                     "zong", "zop", "zoan", "zofia", "zoodle", "zoinks", "zowwys",
                     "zootropic",
                     "stoll", "steele","steamstream", "stuper","stong", "stupify", "steale", "stickly",
                     "stats", "stat", "steeler",
                     "protype", "pringle", "preporation", "preech", "pradae", "proactive", "practicum",
                     "priranha", "primede", "prine", "priviledge", "privelege", "priere", "pratt",
                     "predjudice", "pread", "procer", "prad", "probbe", "protone", "pronation", "prada",
                     "proffesional", "promo",
                     "mifted", "milf", "misadministration", "microplane", "milkshake", "minnce", 
                     "millineter", "mit", "milor", "mitochondria", "minging", "millisecond", "millow",
                     "mittle", "mior","mich", "misunderstood", "miniatrure",
                     "leep", "lemer", "leem", "leen", "leahc", "leavor", "lepper", "letal", "leb",
                     "leir", "lerch", "lego",
                     "gray", "grinch", "greive", "grampa", "grungy", "grunge", "grandeous", "grap",
                     "grilla", "gret", "groot", "groope", "grimm", "growed",
                     "gluck", "glaven", "glute", "glag", "glicemic", "gle", "glam", "gleen", "glamping",
                     "glimp", "glack", "gloitter", "glintstone", "glep", "gleem", "glock", "glot",
                     "glay", "glup", "glantern", "glurp",
                     "eannagram", "eaw", "eaze", "eatily", "eal", "east", "eavesrop", "eads", "eab",
                     "eap", "eather", "easports", 
                     "dagwood", "dat", "dans", "dawg", "dain", "datamine", "daquirri", "dask", "dallup",
                     "daith", "dack", "dax", "dall", "daly",
                     "abcess", "abs", "abba", "abscent", "abhorent", "abalon", "abott", "abcs", "abraid",
                     "abling", "abt", "abrash", "abdomnial", "abled", "abdjure", "abdominals", "aboot",
                     "abbhor", "abondant", "abicuss", "abrail", "ablebody", "abscomb", "abid", "abduction",
                     "abor", "abeed", "abacist" 
                     )

# Adding words that considered non-word by dictionary's default: #SM# it seems you are adding TRUE words that were not included in the dictionary's default database! This comment could be improved.
dict <- c(qdapDictionaries::GradyAugmented, words_not_in_dic)

# Removing non-words that considered words by dictionary's default: 
dict = dict[!dict %in% c("st", "mi", "da")]

# Function used to check whether a given word exists in our dictionary:
is.word <- function(word, dictionary) {
  tolower(word) %in% dictionary
}
```


## Data processing
```{r}
# Data processing pipeline
stem_task_data <- clean_response(master_file, 'let_diverse|let_linear', c("stimulus")) %>% 
  mutate(letter_set = tolower(substr(response, start = 1, stop = 2))) %>% # Creates a new column 'letter_set' by converting the first two letters of 'response' to lowercase.
  filter(grepl('ab|da|ea|gl|gr|le|mi|pr|st|zo', letter_set, ignore.case = TRUE)) %>% # Filters rows where 'letter_set' matches any of the specified two-letter combinations, case-insensitive.
  filter(is.word(response, dict)) %>%  # Further filters responses based on whether they are valid words according to a specified dictionary ('dict').
  select(c('subject_id', 'time_elapsed', 'rt', 'response', 'letter_set')) # Selects only the specified columns to keep in the final dataset.

```

## Word Fz Dataset (COCA) 60k FreeDataSet from: https://www.wordfrequency.info/samples.asp 
#SM# Good job in making the source available here
```{r}
df_word_freq <- read_excel('data/wordFrequency60k.xlsx', sheet = 'lemmas')  %>% 
  select(c('rank','lemma','freq'))
```

## ---------- Main Research Questions ---------------

Relationship between Time Response and Word Frequency (done) 

Relationship between Time Response and Semantic Similarity (work in progress)


## Merge the datasets by the word
This is the dataset that we are going to use for the analysis
```{r}
combined_data <- merge(stem_task_data, df_word_freq, by.x = "response", by.y = "lemma")
```

## Basic Stats
```{r}
# Number of Unique Subjects
num_subjects <- length(unique(master_file$subject_id))
cat("Number of Subjects:", num_subjects, "\n")

# Range of Response Times
min_rt <- min(master_file$rt, na.rm = TRUE)
max_rt <- max(master_file$rt, na.rm = TRUE)
cat("Range of Response Times: Min =", min_rt, ", Max =", max_rt, "\n")

# Mean and Median of Response Time
mean_rt <- mean(master_file$rt, na.rm = TRUE)
median_rt <- median(master_file$rt, na.rm = TRUE)
cat("Mean Response Time:", mean_rt, "\n")
cat("Median Response Time:", median_rt, "\n")

# Standard Deviation of Response Time
sd_rt <- sd(master_file$rt, na.rm = TRUE)
cat("Standard Deviation of Response Time:", sd_rt, "\n")
```

## ---------- Check Normality and Linearity ---------------

Histogram to assess the distribution visually. The red line represents a kernel density estimate of your data. The dotted blue line is a normal distribution curve superimposed on the histogram, and is a visual representation of what your data distribution would look like if it followed a perfect normal distribution.

```{r}
# Histogram with mean and median lines, and x-axis limited to 10000
ggplot(master_file, aes(x = rt)) +
  geom_histogram(aes(y = ..density..), binwidth = 100, fill = "#69b3a2", color = "black") +
  geom_vline(aes(xintercept = mean_rt), color = "blue", linetype = "dashed", size = 1, alpha = 0.7) +
  geom_vline(aes(xintercept = median_rt), color = "green", linetype = "dashed", size = 1, alpha = 0.7) +
  xlim(c(0, 20000)) +
  theme_minimal() +
  geom_density(color = "red", size = 1) +
  stat_function(fun = dnorm, args = list(mean = mean(combined_data$rt, na.rm = TRUE), 
                                         sd = sd(combined_data$rt, na.rm = TRUE)), color = "blue", linetype = "dashed") +
  labs(
    title = "Histogram of Response Time with Mean and Median (Up to 10,000)",
    x = "Response Time",
    y = "Density"
  ) +
  geom_text(aes(x = mean_rt, y = 0, label = paste("Mean =", round(mean_rt, 2))), vjust = 2, color = "blue", hjust = 0, size = 4, alpha = 0.7) +
  geom_text(aes(x = median_rt, y = 0, label = paste("Median =", round(median_rt, 2))), vjust = -20, color = "green", hjust = 0, size = 4, alpha = 0.7)
```

## Q-Q Plots to check for deviations from normality
```{r}
qqnorm(combined_data$rt, main = "Q-Q Plot for Response Time")
qqline(combined_data$rt, col = "red") # Color for visibility

```

```{r}
qqnorm(combined_data$freq, main = "Q-Q Plot for Word Fz")
qqline(combined_data$freq, col = "red") # Color for visibility
```

## Shapiro-Wilk test for normality, we are going to use p.value > 0.05 
```{r}
shapiro.test(combined_data$rt) # Response Time -> W = 0.80598, p-value < 2.2e-16 ----- NO NORMALITY ----
shapiro.test(combined_data$freq) # Frequency -> W = 0.46676, p-value < 2.2e-16 ------- NO NORMALITY ----
```

## ---------------- Word Frequency --------------------

# Hexbin plot to visualize the relationship between Response Time and Word Frequency. 
We don't see any correlation. 

```{r}
ggplot(combined_data, aes(x = freq, y = rt)) +
  geom_hex() +
  labs(title = "Hexbin Plot of Frequency vs. Time Response", x = "Word Frequency", y = "Time Response") +
  theme_minimal()
```

## ---------- Correlation Test ---------------

Both without normality, then we are going to use Spearman. 
No correlation. 
```{r}
cor_test_result <- cor.test(combined_data$rt, combined_data$freq, method="spearman")
print(cor_test_result)
```
# Kendall's tau 
(just in case) but no correlation neither
```{r}
cor.test(combined_data$rt, combined_data$freq, method="kendall")
```
