---
title: "Data Science Project"
author: "David Marcos Cuesta"
date: "2023-12-06"
output: html_document
---

## Libraries
```{r}
library(tidyverse)
library(qdapDictionaries)
## DH: Already loaded with `tidyverse`
library(ggplot2) 
library(readxl)            # Read .xlsx   
library(hexbin) 

## DH: I think this is only necessary if you're knitting to slides
knitr::opts_chunk$set(echo = TRUE)

```


## Function to clean data
```{r}
# Function to clean response text
clean_response <- function(data, pattern, cols_to_remove) {
  data %>% 
    select(-all_of(cols_to_remove)) %>% 
    filter(str_detect(response, pattern)) %>%
    mutate(response = str_replace_all(response, '[\"{}]', '')) %>%
    mutate(response = str_remove_all(response, "let_| ")) %>%
    separate(response, c("condition", "response"), sep = ":")
}
```



## Read the data from the raw dataset
<!-- DH: What is this data?  Where did it come from?  -->
```{r}
## DH: Recommend using `file.path()` or `here()` to construct paths
master_file <- read.csv('data/master_file_01.csv') %>% 
  select(subject_id, stimulus, trial_index, time_elapsed, rt, response)
```


## Define the dictionary of possible English words

<!-- DH: What is this, why do we need it, why these particular words? -->

```{r}
# Function to check if it's a word in the dictionary

words_not_in_dic = c("zoopathology", "zolt", "zoolagist", "zot", "zock", "zook", "zon",
                     "zosh", "zoze", "zoam", "zommer", "zozo", "zoomies", "zoop", "zope",
                     "zoetrope", "zoocology", "zote", "zoge", "zoed", "zomed", "zork",
                     "zomboni", "zotie", "zolac", "zoz", "zoadic", "zooplankton", "zoro",
                     "zong", "zop", "zoan", "zofia", "zoodle", "zoinks", "zowwys",
                     "zootropic",
                     "stoll", "steele","steamstream", "stuper","stong", "stupify", "steale", "stickly",
                     "stats", "stat", "steeler",
                     "protype", "pringle", "preporation", "preech", "pradae", "proactive", "practicum",
                     "priranha", "primede", "prine", "priviledge", "privelege", "priere", "pratt",
                     "predjudice", "pread", "procer", "prad", "probbe", "protone", "pronation", "prada",
                     "proffesional", "promo",
                     "mifted", "milf", "misadministration", "microplane", "milkshake", "minnce", 
                     "millineter", "mit", "milor", "mitochondria", "minging", "millisecond", "millow",
                     "mittle", "mior","mich", "misunderstood", "miniatrure",
                     "leep", "lemer", "leem", "leen", "leahc", "leavor", "lepper", "letal", "leb",
                     "leir", "lerch", "lego",
                     "gray", "grinch", "greive", "grampa", "grungy", "grunge", "grandeous", "grap",
                     "grilla", "gret", "groot", "groope", "grimm", "growed",
                     "gluck", "glaven", "glute", "glag", "glicemic", "gle", "glam", "gleen", "glamping",
                     "glimp", "glack", "gloitter", "glintstone", "glep", "gleem", "glock", "glot",
                     "glay", "glup", "glantern", "glurp",
                     "eannagram", "eaw", "eaze", "eatily", "eal", "east", "eavesrop", "eads", "eab",
                     "eap", "eather", "easports", 
                     "dagwood", "dat", "dans", "dawg", "dain", "datamine", "daquirri", "dask", "dallup",
                     "daith", "dack", "dax", "dall", "daly",
                     "abcess", "abs", "abba", "abscent", "abhorent", "abalon", "abott", "abcs", "abraid",
                     "abling", "abt", "abrash", "abdomnial", "abled", "abdjure", "abdominals", "aboot",
                     "abbhor", "abondant", "abicuss", "abrail", "ablebody", "abscomb", "abid", "abduction",
                     "abor", "abeed", "abacist" 
                     )

# Adding words that considered non-word by dictionary's default: #SM# it seems you are adding TRUE words that were not included in the dictionary's default database! This comment could be improved.
dict <- c(qdapDictionaries::GradyAugmented, words_not_in_dic)

# Removing non-words that considered words by dictionary's default: 
## DH: violates immutability
dict = dict[!dict %in% c("st", "mi", "da")]

# Function used to check whether a given word exists in our dictionary:
is.word <- function(word, dictionary) {
  tolower(word) %in% dictionary
}
```


## Data processing
```{r}
## DH: Super long lines. To make this more readable, put the comment above the line rather than at the end, try shorter comments explaining "why" rather than "what," and add line breaks + deeper indenting 
# Data processing pipeline
stem_task_data <- clean_response(master_file, 'let_diverse|let_linear', c("stimulus")) %>% 
  mutate(letter_set = tolower(substr(response, start = 1, stop = 2))) %>% # Creates a new column 'letter_set' by converting the first two letters of 'response' to lowercase.
  filter(grepl('ab|da|ea|gl|gr|le|mi|pr|st|zo', letter_set, ignore.case = TRUE)) %>% # Filters rows where 'letter_set' matches any of the specified two-letter combinations, case-insensitive.
  filter(is.word(response, dict)) %>%  # Further filters responses based on whether they are valid words according to a specified dictionary ('dict').
  select(c('subject_id', 'time_elapsed', 'rt', 'response', 'letter_set')) # Selects only the specified columns to keep in the final dataset.

```

## Word Fz Dataset (COCA) 60k FreeDataSet from: https://www.wordfrequency.info/samples.asp 
#SM# Good job in making the source available here

<!-- DH: Did you mean to include a URL in a section header? -->

```{r}
df_word_freq <- read_excel('data/wordFrequency60k.xlsx', 
                           sheet = 'lemmas')  %>% 
  select(c('rank','lemma','freq'))
```

## ---------- Main Research Questions ---------------

<!-- DH: Did you mean to include these lines in a section header? -->

Relationship between Time Response and Word Frequency (done) 

Relationship between Time Response and Semantic Similarity (work in progress)

<!-- DH: This is cryptic -->


## Merge the datasets by the word
This is the dataset that we are going to use for the analysis
```{r}
## DH: Long line
combined_data <- merge(stem_task_data, df_word_freq, by.x = "response", by.y = "lemma")
```

## Basic Stats
```{r}
# Number of Unique Subjects
num_subjects <- length(unique(master_file$subject_id))
cat("Number of Subjects:", num_subjects, "\n")

# Range of Response Times
min_rt <- min(master_file$rt, na.rm = TRUE)
max_rt <- max(master_file$rt, na.rm = TRUE)
cat("Range of Response Times: Min =", min_rt, ", Max =", max_rt, "\n")

# Mean and Median of Response Time
mean_rt <- mean(master_file$rt, na.rm = TRUE)
median_rt <- median(master_file$rt, na.rm = TRUE)
cat("Mean Response Time:", mean_rt, "\n")
cat("Median Response Time:", median_rt, "\n")

# Standard Deviation of Response Time
sd_rt <- sd(master_file$rt, na.rm = TRUE)
cat("Standard Deviation of Response Time:", sd_rt, "\n")
```

<!-- DH: At this point, it seems like you skip most of Peng and Matsui's checklist, and move directly to drawing a plot to address your research question. -->

## ---------- Check Normality and Linearity ---------------

Histogram to assess the distribution visually. The red line represents a kernel density estimate of your data. The dotted blue line is a normal distribution curve superimposed on the histogram, and is a visual representation of what your data distribution would look like if it followed a perfect normal distribution.

```{r}
## DH: Long lines, and a little hard to parse with minimal indentation
# Histogram with mean and median lines, and x-axis limited to 10000
ggplot(master_file, aes(x = rt)) +
  geom_histogram(aes(y = ..density..), binwidth = 100, fill = "#69b3a2", color = "black") +
  geom_vline(aes(xintercept = mean_rt), color = "blue", linetype = "dashed", size = 1, alpha = 0.7) +
  geom_vline(aes(xintercept = median_rt), color = "green", linetype = "dashed", size = 1, alpha = 0.7) +
  xlim(c(0, 20000)) +
  theme_minimal() +
  geom_density(color = "red", size = 1) +
  stat_function(fun = dnorm, args = list(mean = mean(combined_data$rt, na.rm = TRUE), 
                                         sd = sd(combined_data$rt, na.rm = TRUE)), color = "blue", linetype = "dashed") +
  labs(
    title = "Histogram of Response Time with Mean and Median (Up to 10,000)",
    x = "Response Time",
    y = "Density"
  ) +
  geom_text(aes(x = mean_rt, y = 0, label = paste("Mean =", round(mean_rt, 2))), vjust = 2, color = "blue", hjust = 0, size = 4, alpha = 0.7) +
  geom_text(aes(x = median_rt, y = 0, label = paste("Median =", round(median_rt, 2))), vjust = -20, color = "green", hjust = 0, size = 4, alpha = 0.7)
```

<!-- DH: Anything of note here?  Is it concerning that 5.6k rows were dropped? --> 


## Q-Q Plots to check for deviations from normality
```{r}
qqnorm(combined_data$rt, main = "Q-Q Plot for Response Time")
qqline(combined_data$rt, col = "red") # Color for visibility

```

```{r}
qqnorm(combined_data$freq, main = "Q-Q Plot for Word Fz")
qqline(combined_data$freq, col = "red") # Color for visibility
```

<!-- DH: Anything notable here? -->

## Shapiro-Wilk test for normality, we are going to use p.value > 0.05 
```{r}
shapiro.test(combined_data$rt) # Response Time -> W = 0.80598, p-value < 2.2e-16 ----- NO NORMALITY ----
shapiro.test(combined_data$freq) # Frequency -> W = 0.46676, p-value < 2.2e-16 ------- NO NORMALITY ----
```

<!-- DH: Did we really need the hypothesis test?  Just visually, they have long right tails.  If we really need normality, maybe look at the logged variables instead?  Or, if we don't need normality, why check for it? -->

## ---------------- Word Frequency --------------------

# Hexbin plot to visualize the relationship between Response Time and Word Frequency. 
We don't see any correlation. 

```{r}
ggplot(combined_data, aes(x = freq, y = rt)) +
  geom_hex() +
  labs(title = "Hexbin Plot of Frequency vs. Time Response", x = "Word Frequency", y = "Time Response") +
  theme_minimal()
```

## ---------- Correlation Test ---------------

Both without normality, then we are going to use Spearman. 
No correlation. 
```{r}
## DH: Lone line
cor_test_result <- cor.test(combined_data$rt, combined_data$freq, method="spearman")
print(cor_test_result)
```
# Kendall's tau 
(just in case) but no correlation neither
```{r}
cor.test(combined_data$rt, combined_data$freq, method="kendall")
```

<!-- DH: And the upshot is ... ? -->
